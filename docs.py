#!/usr/bin/env python3
"""
é™æ€ç½‘ç«™Markdownçˆ¬å–å·¥å…·
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨çˆ¬å–æ•´ä¸ªæ–‡æ¡£ç½‘ç«™
2. ä¿æŒåŸå§‹Markdownæ ¼å¼
3. ä¸‹è½½å›¾ç‰‡å¹¶ä¿æŒæ­£ç¡®å¼•ç”¨
4. ç»´æŒåŸå§‹ç›®å½•ç»“æ„
5. è‡ªåŠ¨å¤„ç†ç›¸å¯¹/ç»å¯¹è·¯å¾„
"""

import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

class MarkdownScraper:
    def __init__(self, base_url, output_dir="docs"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        :param base_url: æ–‡æ¡£ç½‘ç«™åŸºç¡€URL (å¦‚ "https://example.com/docs")
        :param output_dir: è¾“å‡ºç›®å½• (é»˜è®¤ "docs")
        """
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir).resolve()
        self.visited = set()
        self.asset_map = {}  # å­˜å‚¨å·²ä¸‹è½½èµ„æº
        
        # é…ç½®HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_relative_path(self, url):
        """å°†URLè½¬æ¢ä¸ºç›¸å¯¹äºbase_urlçš„è·¯å¾„"""
        return urlparse(url).path.replace(urlparse(self.base_url).path, '').lstrip('/')
        
    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        filename = unquote(filename)
        filename = re.sub(r'[\\/*?:"<>|]', "_", filename)
        return filename[:100]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        
    def download_asset(self, url, asset_type='images'):
        """
        ä¸‹è½½èµ„æºæ–‡ä»¶ï¼ˆå›¾ç‰‡/CSSç­‰ï¼‰
        è¿”å›æœ¬åœ°ç›¸å¯¹è·¯å¾„
        """
        if url in self.asset_map:
            return self.asset_map[url]
    
        try:
            asset_dir = self.output_dir / asset_type
            asset_dir.mkdir(parents=True, exist_ok=True)
        
            # ä» URL ä¸­æå–åŸå§‹æ–‡ä»¶å
            parsed = urlparse(url)
            # å¯¹è·¯å¾„è¿›è¡Œè§£ç ï¼Œå†æå–æ–‡ä»¶å
            orig_filename = os.path.basename(unquote(parsed.path))
            if not orig_filename:
                # å¦‚æå–ä¸åˆ°ï¼Œåˆ™é€€å›ä½¿ç”¨ MD5 åç§°
                ext = os.path.splitext(parsed.path)[1][1:] or 'bin'
                filename = f"{hashlib.md5(url.encode()).hexdigest()[:8]}.{ext}"
            else:
                filename = self._sanitize_filename(orig_filename)
        
            save_path = asset_dir / filename
        
            if not save_path.exists():
                response = self.session.get(url, stream=True, timeout=15)
                response.raise_for_status()
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
        
            relative_path = f"{asset_type}/{filename}"
            self.asset_map[url] = relative_path
            return relative_path
        
        except Exception as e:
            print(f"âš ï¸ èµ„æºä¸‹è½½å¤±è´¥: {url} - {e}")
            return url  # è¿”å›åŸå§‹URLä½œä¸ºfallback
            
    def convert_html_to_markdown(self, html, page_url):
        """
        å°†HTMLè½¬æ¢ä¸ºMarkdownå¹¶å¤„ç†èµ„æºå¼•ç”¨
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼Œä¾‹å¦‚ scriptã€styleã€navã€footerã€iframeã€svg
        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'svg']):
            element.decompose()
        
        # ç§»é™¤ <header class="navbar">
        for header in soup.find_all("header", class_="navbar"):
            header.decompose()
    
        # ç§»é™¤ <aside class="sidebar">
        for aside in soup.find_all("aside", class_="sidebar"):
            aside.decompose()
    
        # å¤„ç†å›¾ç‰‡
        for img in soup.find_all('img', src=True):
            img_url = urljoin(page_url, img['src'])
            local_path = self.download_asset(img_url, 'images')
            img['src'] = local_path
            
        # å¤„ç†ä»£ç å—ï¼ˆä¿ç•™Markdownæ ¼å¼ï¼‰
        for pre in soup.find_all('pre'):
            code = pre.get_text()
            pre.replace_with(f"```\n{code}\n```")
            
        from html2text import HTML2Text
        h = HTML2Text()
        h.body_width = 0
        h.mark_code = True
        h.protect_links = True
        markdown = h.handle(str(soup))
        
        # æ¸…ç†æ ¼å¼
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        markdown = re.sub(r'(!\[.*?\])\((\S+)\)', lambda m: f"{m.group(1)}({m.group(2)})", markdown)
        return markdown
        
    def save_markdown_file(self, content, url):
        """
        ä¿å­˜Markdownæ–‡ä»¶å¹¶ä¿æŒç›®å½•ç»“æ„ï¼ŒåŒæ—¶è°ƒæ•´å›¾ç‰‡å¼•ç”¨è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        rel_path = self._get_relative_path(url)
        if not rel_path or rel_path.endswith('/'):
            rel_path += 'index'
        rel_path = re.sub(r'\.(html|htm|php|aspx)$', '', rel_path)
        md_path = (self.output_dir / rel_path).with_suffix('.md')
        md_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ·»åŠ å…ƒä¿¡æ¯
        final_content = f"<!-- æºåœ°å€: {url} -->\n\n{content}"
        
        # è°ƒæ•´å›¾ç‰‡å¼•ç”¨è·¯å¾„ï¼šè®¡ç®— Markdown æ–‡ä»¶æ‰€åœ¨ç›®å½•åˆ°è¾“å‡ºæ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„
        def adjust_img(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            # è®¡ç®—å›¾ç‰‡ç»å¯¹è·¯å¾„ï¼ˆè¾“å‡ºç›®å½•ä¸‹çš„ä½ç½®ï¼‰
            abs_img_path = Path(self.output_dir) / img_path
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼šMarkdown æ–‡ä»¶çš„çˆ¶ç›®å½•åˆ°å›¾ç‰‡æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
            rel_img_path = os.path.relpath(abs_img_path, start=md_path.parent)
            return f"![{alt_text}]({rel_img_path})"
        
        final_content = re.sub(r'!\[(.*?)\]\((.*?)\)', adjust_img, final_content)
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
            
        print(f"âœ… å·²ä¿å­˜: {md_path.relative_to(self.output_dir)}")
        return md_path
        
    def process_page(self, url):
        """
        å¤„ç†å•ä¸ªé¡µé¢
        è¿”å›æœ¬é¡µä¸­æ‰¾åˆ°çš„æ–°é“¾æ¥
        """
        if url in self.visited:
            return set()
            
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {url}")
        self.visited.add(url)
        
        try:
            # è·å–é¡µé¢å†…å®¹
            response = self.session.get(url, timeout=15)
            response.encoding = response.apparent_encoding
            
            # å¤„ç†é‡å®šå‘
            if response.history:
                url = response.url
                
            # è½¬æ¢ä¸ºMarkdown
            md_content = self.convert_html_to_markdown(response.text, url)
            
            # ä¿å­˜æ–‡ä»¶
            self.save_markdown_file(md_content, url)
            
            # æå–æœ¬é¡µé“¾æ¥
            soup = BeautifulSoup(response.text, 'html.parser')
            new_links = set()
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urljoin(url, href)
                
                # åªå¤„ç†åŒåŸŸåä¸‹çš„é“¾æ¥
                if urlparse(full_url).netloc == urlparse(self.base_url).netloc:
                    # ç§»é™¤é”šç‚¹å’ŒæŸ¥è¯¢å‚æ•°
                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url not in self.visited:
                        new_links.add(clean_url)
                        
            return new_links
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url} - {e}")
            return set()
            
    def crawl(self, start_url=None, max_workers=3, delay=0.5):
        """
        å¼€å§‹çˆ¬å–æ•´ä¸ªç½‘ç«™
        :param start_url: èµ·å§‹URL (é»˜è®¤ä½¿ç”¨base_url)
        :param max_workers: å¹¶å‘çº¿ç¨‹æ•°
        :param delay: è¯·æ±‚é—´éš”(ç§’)
        """
        start_url = start_url or self.base_url
        self.visited = set()
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {self.base_url}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # åˆå§‹ä»»åŠ¡
            future_to_url = {
                executor.submit(self.process_page, start_url): start_url
            }
            
            while future_to_url:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        new_links = future.result()
                        # æ·»åŠ æ–°ä»»åŠ¡
                        for link in new_links:
                            if link not in self.visited:
                                time.sleep(delay)
                                future_to_url[
                                    executor.submit(self.process_page, link)
                                ] = link
                    except Exception as e:
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {url} - {e}")
                    finally:
                        del future_to_url[future]
                        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(self.visited)} ä¸ªé¡µé¢")
        print(f"ğŸ“ Markdownæ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿å­˜åœ¨: {self.output_dir}/images")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='é™æ€æ–‡æ¡£ç½‘ç«™Markdownçˆ¬å–å·¥å…·')
    parser.add_argument('url', help='æ–‡æ¡£ç½‘ç«™åŸºç¡€URL (å¦‚ "https://iot.mi.com/vela/quickapp/zh")')
    parser.add_argument('-o', '--output', default='docs', help='è¾“å‡ºç›®å½• (é»˜è®¤: docs)')
    parser.add_argument('-s', '--start', help='èµ·å§‹URL (é»˜è®¤ä½¿ç”¨åŸºç¡€URL)')
    parser.add_argument('-w', '--workers', type=int, default=16, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 3)')
    parser.add_argument('-d', '--delay', type=float, default=0.3, help='è¯·æ±‚é—´éš”(ç§’) (é»˜è®¤: 0.5)')
    
    args = parser.parse_args()
    
    scraper = MarkdownScraper(
        base_url=args.url,
        output_dir=args.output
    )
    
    scraper.crawl(
        start_url=args.start,
        max_workers=args.workers,
        delay=args.delay
    )