#!/usr/bin/env python3
"""
é™æ€ç½‘ç«™Markdownçˆ¬å–å·¥å…·ï¼ˆå°ç±³Velaæ–‡æ¡£ä¸“ç”¨ä¼˜åŒ–ç‰ˆï¼‰
ä¸»è¦ä¿®å¤ï¼š
1. æ ‡é¢˜é”šç‚¹é—®é¢˜ï¼ˆç§»é™¤å¤šä½™çš„#ç¬¦å·ï¼‰
2. ä»£ç å—æ ¼å¼é—®é¢˜ï¼ˆä¿®å¤ç©ºæ ¼å’Œè¯­æ³•é«˜äº®ï¼‰
3. è¡¨æ ¼å¯¹é½é—®é¢˜
4. æ•´ä½“æ’ç‰ˆä¼˜åŒ–
"""

import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

class MarkdownScraper:
    def __init__(self, base_url, output_dir="docs"):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir).resolve()
        self.visited = set()
        self.asset_map = {}
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache'
        })
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def _get_relative_path(self, url):
        return urlparse(url).path.replace(urlparse(self.base_url).path, '').lstrip('/')
        
    def _sanitize_filename(self, filename):
        filename = unquote(filename)
        return re.sub(r'[\\/*?:"<>|]', "_", filename)[:100]
        
    def download_asset(self, url, asset_type='images'):
        if url in self.asset_map:
            return self.asset_map[url]
    
        try:
            asset_dir = self.output_dir / asset_type
            asset_dir.mkdir(parents=True, exist_ok=True)
        
            parsed = urlparse(url)
            orig_filename = os.path.basename(unquote(parsed.path))
            if not orig_filename:
                ext = os.path.splitext(parsed.path)[1][1:] or 'bin'
                filename = f"{hashlib.md5(url.encode()).hexdigest()[:8]}.{ext}"
            else:
                filename = self._sanitize_filename(orig_filename)
        
            save_path = asset_dir / filename
        
            if not save_path.exists():
                response = self.session.get(url, stream=True, timeout=15)
                response.raise_for_status()
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
        
            relative_path = f"{asset_type}/{filename}"
            self.asset_map[url] = relative_path
            return relative_path
        
        except Exception as e:
            print(f"âš ï¸ èµ„æºä¸‹è½½å¤±è´¥: {url} - {e}")
            return url
            
    def _clean_markdown(self, markdown):
        """å°ç±³Velaæ–‡æ¡£ä¸“ç”¨æ¸…ç†å‡½æ•°"""
        # ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼ˆç§»é™¤é”šç‚¹#å·ï¼‰
        markdown = re.sub(r'^#\s+#\s+(.*)$', r'## \1', markdown, flags=re.MULTILINE)
        
        # ä¿®å¤ä»£ç å—æ ¼å¼
        markdown = re.sub(r'```(\w+)\s+', r'```\1\n', markdown)
        markdown = re.sub(r'(\S)\s+```', r'\1\n```', markdown)
        
        # ä¿®å¤æ–¹æ³•è°ƒç”¨ä¸­çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', markdown)
        
        # ä¿®å¤æ‹¬å·å†…çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'\(\s+', '(', markdown)
        markdown = re.sub(r'\s+\)', ')', markdown)
        
        # ä¿®å¤è¡¨æ ¼å¯¹é½
        markdown = re.sub(r'\|(\s*-+\s*)\|', r'|:---:|', markdown)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        
        return markdown
            
    def convert_html_to_markdown(self, html, page_url):
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'svg']):
            element.decompose()
        
        # ç§»é™¤ç‰¹å®šç»„ä»¶
        for tag in ['header.navbar', 'aside.sidebar', 'div.page-nav', 'div.toc']:
            for element in soup.select(tag):
                element.decompose()

        # ç‰¹æ®Šå¤„ç†æ ‡é¢˜ï¼ˆç§»é™¤é”šç‚¹é“¾æ¥ï¼‰
        for header in soup.find_all(re.compile('^h[1-6]$')):
            if header.find('a', class_='header-anchor'):
                header.a.decompose()
                header_text = header.get_text().strip()
                header.string = header_text

        # å¤„ç†ä»£ç å—
        for pre in soup.find_all('pre'):
            parent_div = pre.find_parent('div', class_=re.compile('language-'))
            if parent_div:
                # æå–è¯­è¨€ç±»å‹
                lang_match = re.search(r'language-(\w+)', ' '.join(parent_div['class']))
                language = lang_match.group(1) if lang_match else ''
                
                # æå–åŸå§‹ä»£ç ï¼ˆä¿ç•™æ¢è¡Œå’Œç¼©è¿›ï¼‰
                code = pre.get_text('\n')
                
                # ç‰¹æ®Šå¤„ç†JavaScriptä»£ç 
                if language == 'javascript':
                    code = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', code)
                    code = re.sub(r'\s+\(\s+', '(', code)
                    code = re.sub(r'\s+\)\s+', ')', code)
                
                # æ›¿æ¢ä¸ºMarkdownä»£ç å—
                pre.replace_with(f"```{language}\n{code}\n```")

        # å¤„ç†å›¾ç‰‡
        for img in soup.find_all('img', src=True):
            img_url = urljoin(page_url, img['src'])
            local_path = self.download_asset(img_url, 'images')
            img['src'] = local_path
            
        # ä½¿ç”¨html2textè½¬æ¢
        from html2text import HTML2Text
        h = HTML2Text()
        h.body_width = 0
        h.mark_code = True
        h.protect_links = True
        markdown = h.handle(str(soup))
        
        # åå¤„ç†æ¸…ç†
        markdown = self._clean_markdown(markdown)
        return markdown
        
    def save_markdown_file(self, content, url):
        rel_path = self._get_relative_path(url)
        if not rel_path or rel_path.endswith('/'):
            rel_path += 'index'
        rel_path = re.sub(r'\.(html|htm|php|aspx)$', '', rel_path)
        md_path = (self.output_dir / rel_path).with_suffix('.md')
        md_path.parent.mkdir(parents=True, exist_ok=True)
        
        final_content = f"<!-- æºåœ°å€: {url} -->\n\n{content}"
        
        def adjust_img(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            abs_img_path = Path(self.output_dir) / img_path
            rel_img_path = os.path.relpath(abs_img_path, start=md_path.parent)
            return f"![{alt_text}]({rel_img_path})"
        
        final_content = re.sub(r'!\[(.*?)\]\((.*?)\)', adjust_img, final_content)
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
            
        print(f"âœ… å·²ä¿å­˜: {md_path.relative_to(self.output_dir)}")
        return md_path
        
    def process_page(self, url):
        if url in self.visited:
            return set()
            
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {url}")
        self.visited.add(url)
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = response.apparent_encoding
            
            if response.history:
                url = response.url
                
            md_content = self.convert_html_to_markdown(response.text, url)
            self.save_markdown_file(md_content, url)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            new_links = set()
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urljoin(url, href)
                
                if urlparse(full_url).netloc == urlparse(self.base_url).netloc:
                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url not in self.visited:
                        new_links.add(clean_url)
                        
            return new_links
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url} - {e}")
            return set()
            
    def crawl(self, start_url=None, max_workers=16, delay=0.3):
        start_url = start_url or self.base_url
        self.visited = set()
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {self.base_url}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.process_page, start_url): start_url}
            
            while future_to_url:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        new_links = future.result()
                        for link in new_links:
                            if link not in self.visited:
                                time.sleep(delay)
                                future_to_url[executor.submit(self.process_page, link)] = link
                    except Exception as e:
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {url} - {e}")
                    finally:
                        del future_to_url[future]
                        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(self.visited)} ä¸ªé¡µé¢")
        print(f"ğŸ“ Markdownæ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿å­˜åœ¨: {self.output_dir}/images")

if __name__ == "__main__":
    DEFAULT_URL = "https://iot.mi.com/vela/quickapp/zh"
    
    parser = argparse.ArgumentParser(description='å°ç±³Velaæ–‡æ¡£Markdownçˆ¬å–å·¥å…·')
    parser.add_argument('url', nargs='?', default=DEFAULT_URL, 
                       help=f'æ–‡æ¡£ç½‘ç«™åŸºç¡€URL (é»˜è®¤: {DEFAULT_URL})')
    parser.add_argument('-o', '--output', default='docs', help='è¾“å‡ºç›®å½• (é»˜è®¤: docs)')
    parser.add_argument('-s', '--start', help='èµ·å§‹URL (é»˜è®¤ä½¿ç”¨åŸºç¡€URL)')
    parser.add_argument('-w', '--workers', type=int, default=16, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 16)')
    parser.add_argument('-d', '--delay', type=float, default=0.3, help='è¯·æ±‚é—´éš”(ç§’) (é»˜è®¤: 0.3)')
    
    args = parser.parse_args()
    
    scraper = MarkdownScraper(
        base_url=args.url,
        output_dir=args.output
    )
    
    scraper.crawl(
        start_url=args.start,
        max_workers=args.workers,
        delay=args.delay
    )