#!/usr/bin/env python3
"""
é™æ€ç½‘ç«™Markdownçˆ¬å–å·¥å…·ï¼ˆå°ç±³Velaæ–‡æ¡£ä¸“ç”¨ä¼˜åŒ–ç‰ˆï¼‰
ä¸»è¦ä¿®å¤ï¼š
1. æ ‡é¢˜é”šç‚¹é—®é¢˜ï¼ˆç§»é™¤å¤šä½™çš„#ç¬¦å·ï¼‰
2. ä»£ç å—æ ¼å¼é—®é¢˜ï¼ˆä¿®å¤ç©ºæ ¼å’Œè¯­æ³•é«˜äº®ï¼‰
3. è¡¨æ ¼å¯¹é½é—®é¢˜
4. æ•´ä½“æ’ç‰ˆä¼˜åŒ–
"""

import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

class MarkdownScraper:
    def __init__(self, base_url, output_dir="docs"):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir).resolve()

        # --- æ–°å¢é€»è¾‘ï¼šæ ¹æ® base_url ç¡®å®šå­ç›®å½• ---
        parsed_base = urlparse(self.base_url)
        path_parts = parsed_base.path.strip('/').split('/')
        # å‡è®¾è¯­è¨€æ ‡è¯†ç¬¦æ˜¯ URL è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œä¸”æ˜¯ 'zh' æˆ– 'en'
        self.subdir = ''
        if path_parts and path_parts[-1] in ['zh', 'en']:
            self.subdir = path_parts[-1]
        # ----------------------------

        # å¦‚æœå­˜åœ¨å­ç›®å½•ï¼Œåˆ™è°ƒæ•´è¾“å‡ºè·¯å¾„
        if self.subdir:
            self.output_dir = self.output_dir / self.subdir

        self.visited = set()
        self.asset_map = {}

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache'
        })

        # ç¡®ä¿æœ€ç»ˆçš„è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _get_relative_path(self, url):
        # è®¡ç®—ç›¸å¯¹äº base_url çš„è·¯å¾„ï¼Œä½†ä¸åŒ…å« base_url ä¸­çš„è¯­è¨€éƒ¨åˆ†
        # ä¾‹å¦‚ï¼Œå¦‚æœ base_url æ˜¯ https://iot.mi.com/vela/quickapp/zh/  ï¼Œurl æ˜¯ https://iot.mi.com/vela/quickapp/zh/guide/start
        # åˆ™ path æ˜¯ /vela/quickapp/zh/guide/startï¼Œbase_path æ˜¯ /vela/quickapp/zh
        # ç»“æœæ˜¯ /guide/startï¼Œå»æ‰å¼€å¤´çš„ / å˜æˆ guide/start
        parsed_url = urlparse(url)
        parsed_base = urlparse(self.base_url)
        path = parsed_url.path
        base_path = parsed_base.path
        # ç§»é™¤ base_path åœ¨ path ä¸­çš„å‰ç¼€éƒ¨åˆ†
        rel_path = path.replace(base_path, '', 1).lstrip('/')
        return rel_path

    def _sanitize_filename(self, filename):
        filename = unquote(filename)
        return re.sub(r'[\\/*?:"<>|]', "_", filename)[:100]

    def download_asset(self, url, asset_type='images'):
        if url in self.asset_map:
            return self.asset_map[url]

        try:
            asset_dir = self.output_dir / asset_type
            asset_dir.mkdir(parents=True, exist_ok=True)

            parsed = urlparse(url)
            orig_filename = os.path.basename(unquote(parsed.path))
            if not orig_filename:
                ext = os.path.splitext(parsed.path)[1][1:] or 'bin'
                filename = f"{hashlib.md5(url.encode()).hexdigest()[:8]}.{ext}"
            else:
                filename = self._sanitize_filename(orig_filename)

            save_path = asset_dir / filename

            if not save_path.exists():
                response = self.session.get(url, stream=True, timeout=15)
                response.raise_for_status()
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)

            relative_path = f"{asset_type}/{filename}"
            self.asset_map[url] = relative_path
            return relative_path

        except Exception as e:
            print(f"âš ï¸ èµ„æºä¸‹è½½å¤±è´¥: {url} - {e}")
            return url

    def _clean_markdown(self, markdown):
        """å°ç±³Velaæ–‡æ¡£ä¸“ç”¨æ¸…ç†å‡½æ•°"""
        # ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼ˆç§»é™¤é”šç‚¹#å·ï¼‰
        markdown = re.sub(r'^#\s+#\s+(.*)$', r'## \1', markdown, flags=re.MULTILINE)

        # ä¿®å¤ä»£ç å—æ ¼å¼
        markdown = re.sub(r'```(\w+)\s+', r'```\1\n', markdown)
        markdown = re.sub(r'(\S)\s+```', r'\1\n```', markdown)

        # ä¿®å¤æ–¹æ³•è°ƒç”¨ä¸­çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', markdown)

        # ä¿®å¤æ‹¬å·å†…çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'\(\s+', '(', markdown)
        markdown = re.sub(r'\s+\)', ')', markdown)

        # ä¿®å¤è¡¨æ ¼å¯¹é½
        markdown = re.sub(r'\|(\s*-+\s*)\|', r'|:---:|', markdown)

        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)

        return markdown

    def convert_html_to_markdown(self, html, page_url):
        soup = BeautifulSoup(html, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'svg']):
            element.decompose()

        # ç§»é™¤ç‰¹å®šç»„ä»¶
        for tag in ['header.navbar', 'aside.sidebar', 'div.page-nav', 'div.toc']:
            for element in soup.select(tag):
                element.decompose()

        # ç‰¹æ®Šå¤„ç†æ ‡é¢˜ï¼ˆç§»é™¤é”šç‚¹é“¾æ¥ï¼‰
        for header in soup.find_all(re.compile('^h[1-6]$')):
            if header.find('a', class_='header-anchor'):
                header.a.decompose()
                header_text = header.get_text().strip()
                header.string = header_text

        # å¤„ç†ä»£ç å—
        for pre in soup.find_all('pre'):
            parent_div = pre.find_parent('div', class_=re.compile('language-'))
            if parent_div:
                # æå–è¯­è¨€ç±»å‹
                lang_match = re.search(r'language-(\w+)', ' '.join(parent_div['class']))
                language = lang_match.group(1) if lang_match else ''

                # æå–åŸå§‹ä»£ç ï¼ˆä¿ç•™æ¢è¡Œå’Œç¼©è¿›ï¼‰
                code = pre.get_text('\n')

                # ç‰¹æ®Šå¤„ç†JavaScriptä»£ç 
                if language == 'javascript':
                    code = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', code)
                    code = re.sub(r'\s+\(\s+', '(', code)
                    code = re.sub(r'\s+\)\s+', ')', code)

                # æ›¿æ¢ä¸ºMarkdownä»£ç å—
                pre.replace_with(f"```{language}\n{code}\n```")

        # å¤„ç†å›¾ç‰‡
        for img in soup.find_all('img', src=True):
            img_url = urljoin(page_url, img['src'])
            local_path = self.download_asset(img_url, 'images')
            img['src'] = local_path

        # ä½¿ç”¨html2textè½¬æ¢
        from html2text import HTML2Text
        h = HTML2Text()
        h.body_width = 0
        h.mark_code = True
        h.protect_links = True
        markdown = h.handle(str(soup))

        # åå¤„ç†æ¸…ç†
        markdown = self._clean_markdown(markdown)
        return markdown

    def save_markdown_file(self, content, url):
        rel_path = self._get_relative_path(url)
        if not rel_path or rel_path.endswith('/'):
            rel_path += 'index'
        rel_path = re.sub(r'\.(html|htm|php|aspx)$', '', rel_path)
        md_path = (self.output_dir / rel_path).with_suffix('.md')
        md_path.parent.mkdir(parents=True, exist_ok=True)

        final_content = f"<!-- æºåœ°å€: {url} -->\n\n{content}"

        def adjust_img(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            abs_img_path = Path(self.output_dir) / img_path
            rel_img_path = os.path.relpath(abs_img_path, start=md_path.parent)
            return f"![{alt_text}]({rel_img_path})"

        final_content = re.sub(r'!\[(.*?)\]\((.*?)\)', adjust_img, final_content)

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(final_content)

        print(f"âœ… å·²ä¿å­˜: {md_path.relative_to(self.output_dir)}")
        return md_path

    def process_page(self, url):
        if url in self.visited:
            return set()

        print(f"ğŸ”„ æ­£åœ¨å¤„ç†: {url}")
        self.visited.add(url)

        try:
            response = self.session.get(url, timeout=500)
            response.encoding = response.apparent_encoding

            if response.history:
                url = response.url

            md_content = self.convert_html_to_markdown(response.text, url)
            self.save_markdown_file(md_content, url)

            soup = BeautifulSoup(response.text, 'html.parser')
            new_links = set()

            # --- ä¿®æ”¹é“¾æ¥å‘ç°é€»è¾‘ ---
            # è·å–å½“å‰ scraper å®ä¾‹çš„åŸºç¡€è·¯å¾„ï¼Œç”¨äºéªŒè¯å‘ç°çš„é“¾æ¥
            expected_base_path = urlparse(self.base_url).path # ä¾‹å¦‚: /vela/quickapp/zh/

            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urljoin(url, href)

                parsed_full = urlparse(full_url)
                parsed_base = urlparse(self.base_url)

                # æ£€æŸ¥åŸŸåæ˜¯å¦ç›¸åŒ
                if parsed_full.netloc == parsed_base.netloc:
                    # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥å½“å‰ scraper çš„åŸºç¡€è·¯å¾„å¼€å¤´
                    if not parsed_full.path.startswith(expected_base_path):
                        # å¦‚æœå‘ç°çš„é“¾æ¥ä¸æ˜¯ä»¥å½“å‰ scraper çš„è¯­è¨€è·¯å¾„å¼€å¤´ï¼Œåˆ™è·³è¿‡
                        continue

                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url not in self.visited:
                        new_links.add(clean_url)

            return new_links

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url} - {e}")
            return set()

    def crawl(self, start_url=None, max_workers=16, delay=0.3):
        start_url = start_url or self.base_url
        self.visited = set()

        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {start_url}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.process_page, start_url): start_url}

            while future_to_url:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        new_links = future.result()
                        for link in new_links:
                            if link not in self.visited:
                                time.sleep(delay)
                                future_to_url[executor.submit(self.process_page, link)] = link
                    except Exception as e:
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {url} - {e}")
                    finally:
                        del future_to_url[future]

        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(self.visited)} ä¸ªé¡µé¢")
        print(f"ğŸ“ Markdownæ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿å­˜åœ¨: {self.output_dir}/images")

if __name__ == "__main__":
    # ä¿®æ­£é»˜è®¤ URL ä¸ºåŸºç¡€è·¯å¾„ (ç§»é™¤æœ«å°¾ç©ºæ ¼)
    DEFAULT_URL = "https://iot.mi.com/vela/quickapp/"

    # ä¸å†ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œç›´æ¥ç¡¬ç¼–ç 
    base_url = DEFAULT_URL
    output_dir = "docs"
    delay = 0.3
    workers = 512 # ä½¿ç”¨åˆç†çš„å¹¶å‘æ•°

    languages = ['zh', 'en'] # è¦çˆ¬å–çš„è¯­è¨€ç‰ˆæœ¬

    for lang in languages:
        lang_url = f"{base_url}{lang}/"
        print(f"\n--- å¼€å§‹çˆ¬å– {lang.upper()} ç‰ˆæœ¬ ---")
        scraper = MarkdownScraper(
            base_url=lang_url, # ä½¿ç”¨å¸¦è¯­è¨€çš„ URL ä½œä¸ºåŸºç¡€ URL
            output_dir=output_dir # ä½¿ç”¨æ€»è¾“å‡ºç›®å½•
        )
        scraper.crawl(
            start_url=lang_url, # ä»å¸¦è¯­è¨€çš„ URL å¼€å§‹
            max_workers=workers,
            delay=delay
        )
        print(f"--- {lang.upper()} ç‰ˆæœ¬çˆ¬å–å®Œæˆ ---\n")

    print("ğŸ‰ æ‰€æœ‰è¯­è¨€ç‰ˆæœ¬çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“ Markdownæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¿å­˜åœ¨: {output_dir}/zh/images å’Œ {output_dir}/en/images")
