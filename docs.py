#!/usr/bin/env python3
"""
é™æ€ç½‘ç«™Markdownçˆ¬å–å·¥å…·ï¼ˆå°ç±³Velaæ–‡æ¡£ä¸“ç”¨ä¼˜åŒ–ç‰ˆï¼‰
ä¸»è¦ä¿®å¤ï¼š
1. æ ‡é¢˜é”šç‚¹é—®é¢˜ï¼ˆç§»é™¤å¤šä½™çš„#ç¬¦å·ï¼‰
2. ä»£ç å—æ ¼å¼é—®é¢˜ï¼ˆä¿®å¤ç©ºæ ¼å’Œè¯­æ³•é«˜äº®ï¼‰
3. è¡¨æ ¼å¯¹é½é—®é¢˜
4. æ•´ä½“æ’ç‰ˆä¼˜åŒ–
"""

import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
from datetime import datetime
import threading
import sys


class ColoredOutput:
    """ç”¨äºç¾åŒ–è¾“å‡ºçš„ç±»"""
    # ANSI è½¬ä¹‰åºåˆ—
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

    @staticmethod
    def red(text):
        return f"{ColoredOutput.RED}{text}{ColoredOutput.RESET}"

    @staticmethod
    def green(text):
        return f"{ColoredOutput.GREEN}{text}{ColoredOutput.RESET}"

    @staticmethod
    def yellow(text):
        return f"{ColoredOutput.YELLOW}{text}{ColoredOutput.RESET}"

    @staticmethod
    def blue(text):
        return f"{ColoredOutput.BLUE}{text}{ColoredOutput.RESET}"

    @staticmethod
    def magenta(text):
        return f"{ColoredOutput.MAGENTA}{text}{ColoredOutput.RESET}"

    @staticmethod
    def cyan(text):
        return f"{ColoredOutput.CYAN}{text}{ColoredOutput.RESET}"

    @staticmethod
    def bold(text):
        return f"{ColoredOutput.BOLD}{text}{ColoredOutput.RESET}"

    @staticmethod
    def status_processing(text):
        return f"{ColoredOutput.YELLOW}[PROCESSING]{ColoredOutput.RESET} {text}"

    @staticmethod
    def status_success(text):
        return f"{ColoredOutput.GREEN}[SUCCESS]{ColoredOutput.RESET} {text}"

    @staticmethod
    def status_error(text):
        return f"{ColoredOutput.RED}[ERROR]{ColoredOutput.RESET} {text}"

    @staticmethod
    def status_info(text):
        return f"{ColoredOutput.CYAN}[INFO]{ColoredOutput.RESET} {text}"


class RealTimeStats:
    """å®æ—¶ç»Ÿè®¡ä¿¡æ¯ç±»"""
    def __init__(self):
        self.processed_pages = 0
        self.failed_pages = 0
        self.downloaded_assets = 0
        self.currently_processing = ""  # å½“å‰æ­£åœ¨å¤„ç†çš„ URL
        self.lock = threading.Lock()
        self.start_time = datetime.now()

    def update_processing(self, url):
        with self.lock:
            self.currently_processing = url

    def inc_processed(self):
        with self.lock:
            self.processed_pages += 1

    def inc_failed(self):
        with self.lock:
            self.failed_pages += 1

    def inc_assets(self):
        with self.lock:
            self.downloaded_assets += 1

    def get_stats(self):
        with self.lock:
            return {
                'processed': self.processed_pages,
                'failed': self.failed_pages,
                'assets': self.downloaded_assets,
                'current': self.currently_processing,
                'elapsed': datetime.now() - self.start_time
            }

    def display_line(self):
        stats = self.get_stats()
        elapsed_str = str(stats['elapsed']).split('.')[0] # ç§»é™¤å¾®ç§’
        # ä½¿ç”¨ \r å¼€å¤´ï¼Œ\033[K æ¸…é™¤è¡Œå°¾å†…å®¹
        sys.stdout.write(f"\r{ColoredOutput.CYAN}[INFO]{ColoredOutput.RESET} å·²è€—æ—¶: {elapsed_str} | å·²å¤„ç†: {stats['processed']} | å¤±è´¥: {stats['failed']} | èµ„æº: {stats['assets']} | å½“å‰: {stats['current'][:50]}{'...' if len(stats['current']) > 50 else ''}\033[K")
        sys.stdout.flush()


class MarkdownScraper:
    def __init__(self, base_url, output_dir="docs", stats=None):
        self.base_url = base_url.rstrip('/')
        self.output_dir = Path(output_dir).resolve()
        self.stats = stats if stats else RealTimeStats() # æ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„ç»Ÿè®¡å¯¹è±¡

        # --- æ–°å¢é€»è¾‘ï¼šæ ¹æ® base_url ç¡®å®šå­ç›®å½• ---
        parsed_base = urlparse(self.base_url)
        path_parts = parsed_base.path.strip('/').split('/')
        self.subdir = ''
        if path_parts and path_parts[-1] in ['zh', 'en']:
            self.subdir = path_parts[-1]

        # å¦‚æœå­˜åœ¨å­ç›®å½•ï¼Œåˆ™è°ƒæ•´è¾“å‡ºè·¯å¾„
        if self.subdir:
            self.output_dir = self.output_dir / self.subdir

        self.visited = set()
        self.asset_map = {}

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache'
        })

        # ç¡®ä¿æœ€ç»ˆçš„è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _get_relative_path(self, url):
        parsed_url = urlparse(url)
        parsed_base = urlparse(self.base_url)
        path = parsed_url.path
        base_path = parsed_base.path
        rel_path = path.replace(base_path, '', 1).lstrip('/')
        return rel_path

    def _sanitize_filename(self, filename):
        filename = unquote(filename)
        return re.sub(r'[\\/*?:"<>|]', "_", filename)[:100]

    def download_asset(self, url, asset_type='images'):
        if url in self.asset_map:
            return self.asset_map[url]

        try:
            asset_dir = self.output_dir / asset_type
            asset_dir.mkdir(parents=True, exist_ok=True)

            parsed = urlparse(url)
            orig_filename = os.path.basename(unquote(parsed.path))
            if not orig_filename:
                ext = os.path.splitext(parsed.path)[1][1:] or 'bin'
                filename = f"{hashlib.md5(url.encode()).hexdigest()[:8]}.{ext}"
            else:
                filename = self._sanitize_filename(orig_filename)

            save_path = asset_dir / filename

            if not save_path.exists():
                response = self.session.get(url, stream=True, timeout=15)
                response.raise_for_status()
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
                self.stats.inc_assets()
                # ä¸å†æ‰“å°å•ä¸ªèµ„æºä¸‹è½½ä¿¡æ¯ï¼Œä¿æŒè¡Œå†…æ›´æ–°
                # print(ColoredOutput.status_success(f"Asset downloaded: {filename}"))

            relative_path = f"{asset_type}/{filename}"
            self.asset_map[url] = relative_path
            return relative_path

        except Exception as e:
            print() # æ¢è¡Œä»¥é¿å…ä¸è¡Œå†…ç»Ÿè®¡å†²çª
            print(ColoredOutput.status_error(f"èµ„æºä¸‹è½½å¤±è´¥: {url} - {e}"))
            return url

    def _clean_markdown(self, markdown):
        """å°ç±³Velaæ–‡æ¡£ä¸“ç”¨æ¸…ç†å‡½æ•°"""
        # ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼ˆç§»é™¤é”šç‚¹#å·ï¼‰
        markdown = re.sub(r'^#\s+#\s+(.*)$', r'## \1', markdown, flags=re.MULTILINE)

        # ä¿®å¤ä»£ç å—æ ¼å¼
        markdown = re.sub(r'```(\w+)\s+', r'```\1\n', markdown)
        markdown = re.sub(r'(\S)\s+```', r'\1\n```', markdown)

        # ä¿®å¤æ–¹æ³•è°ƒç”¨ä¸­çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', markdown)

        # ä¿®å¤æ‹¬å·å†…çš„å¤šä½™ç©ºæ ¼
        markdown = re.sub(r'\(\s+', '(', markdown)
        markdown = re.sub(r'\s+\)', ')', markdown)

        # ä¿®å¤è¡¨æ ¼å¯¹é½
        markdown = re.sub(r'\|(\s*-+\s*)\|', r'|:---:|', markdown)

        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)

        return markdown

    def convert_html_to_markdown(self, html, page_url):
        soup = BeautifulSoup(html, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'svg']):
            element.decompose()

        # ç§»é™¤ç‰¹å®šç»„ä»¶
        for tag in ['header.navbar', 'aside.sidebar', 'div.page-nav', 'div.toc']:
            for element in soup.select(tag):
                element.decompose()

        # ç‰¹æ®Šå¤„ç†æ ‡é¢˜ï¼ˆç§»é™¤é”šç‚¹é“¾æ¥ï¼‰
        for header in soup.find_all(re.compile('^h[1-6]$')):
            if header.find('a', class_='header-anchor'):
                header.a.decompose()
                header_text = header.get_text().strip()
                header.string = header_text

        # å¤„ç†ä»£ç å—
        for pre in soup.find_all('pre'):
            parent_div = pre.find_parent('div', class_=re.compile('language-'))
            if parent_div:
                lang_match = re.search(r'language-(\w+)', ' '.join(parent_div['class']))
                language = lang_match.group(1) if lang_match else ''
                code = pre.get_text('\n')

                if language == 'javascript':
                    code = re.sub(r'(\w)\s+\.\s+(\w)', r'\1.\2', code)
                    code = re.sub(r'\s+\(\s+', '(', code)
                    code = re.sub(r'\s+\)\s+', ')', code)

                pre.replace_with(f"```{language}\n{code}\n```")

        # å¤„ç†å›¾ç‰‡
        for img in soup.find_all('img', src=True):
            img_url = urljoin(page_url, img['src'])
            local_path = self.download_asset(img_url, 'images')
            img['src'] = local_path

        # ä½¿ç”¨html2textè½¬æ¢
        from html2text import HTML2Text
        h = HTML2Text()
        h.body_width = 0
        h.mark_code = True
        h.protect_links = True
        markdown = h.handle(str(soup))

        # åå¤„ç†æ¸…ç†
        markdown = self._clean_markdown(markdown)
        return markdown

    def save_markdown_file(self, content, url):
        rel_path = self._get_relative_path(url)
        if not rel_path or rel_path.endswith('/'):
            rel_path += 'index'
        rel_path = re.sub(r'\.(html|htm|php|aspx)$', '', rel_path)
        md_path = (self.output_dir / rel_path).with_suffix('.md')
        md_path.parent.mkdir(parents=True, exist_ok=True)

        final_content = f"<!-- æºåœ°å€: {url} -->\n\n{content}"

        def adjust_img(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            abs_img_path = Path(self.output_dir) / img_path
            rel_img_path = os.path.relpath(abs_img_path, start=md_path.parent)
            return f"![{alt_text}]({rel_img_path})"

        final_content = re.sub(r'!\[(.*?)\]\((.*?)\)', adjust_img, final_content)

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(final_content)

        self.stats.inc_processed()
        # ä¸å†æ‰“å°å•ä¸ªæ–‡ä»¶ä¿å­˜ä¿¡æ¯ï¼Œä¿æŒè¡Œå†…æ›´æ–°
        # print(ColoredOutput.status_success(f"Saved: {md_path.relative_to(self.output_dir)}"))
        return md_path

    def process_page(self, url):
        if url in self.visited:
            return set()

        self.stats.update_processing(url) # æ›´æ–°å½“å‰å¤„ç†çš„URL
        # æ³¨æ„ï¼šè¿™é‡Œä¸ç›´æ¥æ‰“å°ï¼Œè€Œæ˜¯é€šè¿‡å®šæ—¶å™¨åœ¨è¡Œå†…æ›´æ–°
        # print(ColoredOutput.status_processing(f"Processing: {url}"))

        self.visited.add(url)

        try:
            response = self.session.get(url, timeout=500)
            response.encoding = response.apparent_encoding

            if response.history:
                url = response.url

            md_content = self.convert_html_to_markdown(response.text, url)
            self.save_markdown_file(md_content, url)

            soup = BeautifulSoup(response.text, 'html.parser')
            new_links = set()

            expected_base_path = urlparse(self.base_url).path

            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urljoin(url, href)

                parsed_full = urlparse(full_url)
                parsed_base = urlparse(self.base_url)

                if parsed_full.netloc == parsed_base.netloc:
                    if not parsed_full.path.startswith(expected_base_path):
                        continue

                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url not in self.visited:
                        new_links.add(clean_url)

            return new_links

        except Exception as e:
            print() # æ¢è¡Œä»¥é¿å…ä¸è¡Œå†…ç»Ÿè®¡å†²çª
            print(ColoredOutput.status_error(f"å¤„ç†å¤±è´¥: {url} - {e}"))
            self.stats.inc_failed()
            return set()


def run_crawler_with_realtime_stats(scraper_instance, start_url, max_workers, delay):
    """åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«ï¼Œå¹¶åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°å®æ—¶ç»Ÿè®¡"""
    import threading
    import time

    stats = scraper_instance.stats
    stop_display = threading.Event()

    def display_loop():
        while not stop_display.is_set():
            stats.display_line()
            time.sleep(0.5) # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡æ˜¾ç¤º

    # å¯åŠ¨æ˜¾ç¤ºçº¿ç¨‹
    display_thread = threading.Thread(target=display_loop, daemon=True)
    display_thread.start()

    # åœ¨å½“å‰çº¿ç¨‹è¿è¡Œçˆ¬è™«ä¸»é€»è¾‘
    start_url = start_url or scraper_instance.base_url
    scraper_instance.visited = set()

    print("\n" + "="*80)
    print(ColoredOutput.bold(ColoredOutput.magenta("ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡")))
    print("="*80)
    print(f"  ç›®æ ‡ URL: {ColoredOutput.cyan(start_url)}")
    print(f"  è¾“å‡ºç›®å½•: {ColoredOutput.cyan(scraper_instance.output_dir)}")
    print(f"  å¹¶å‘çº¿ç¨‹æ•°: {ColoredOutput.yellow(max_workers)}")
    print(f"  è¯·æ±‚é—´éš”: {ColoredOutput.yellow(delay)}s")
    print("-"*80)
    print("  å®æ—¶çŠ¶æ€: ") # å¼€å§‹è¡Œå†…æ›´æ–°

    all_urls_to_process = {start_url}
    processed_urls = set()
    futures_map = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤ç¬¬ä¸€ä¸ªä»»åŠ¡
        futures_map[executor.submit(scraper_instance.process_page, start_url)] = start_url

        while futures_map:
            for future in as_completed(futures_map):
                url = futures_map[future]
                try:
                    new_links = future.result()
                    processed_urls.add(url)
                    
                    for link in new_links:
                        if link not in processed_urls and link not in all_urls_to_process:
                            all_urls_to_process.add(link)
                    
                    submitted_this_round = 0
                    for link in list(all_urls_to_process - processed_urls):
                        if submitted_this_round < max_workers:
                            time.sleep(delay)
                            futures_map[executor.submit(scraper_instance.process_page, link)] = link
                            submitted_this_round += 1
                        else:
                            break

                except Exception as e:
                    print() # æ¢è¡Œ
                    print(ColoredOutput.status_error(f"ä»»åŠ¡å¤±è´¥: {url} - {e}"))
                    stats.inc_failed()
                finally:
                    del futures_map[future]

    # åœæ­¢æ˜¾ç¤ºçº¿ç¨‹
    stop_display.set()
    display_thread.join()

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*80) # æ¢è¡Œå¹¶æ‰“å°åˆ†éš”ç¬¦
    print(ColoredOutput.bold(ColoredOutput.green("âœ… çˆ¬å–ä»»åŠ¡å®Œæˆï¼")))
    print("="*80)
    
    final_stats = stats.get_stats()
    print(f"  å·²å¤„ç†é¡µé¢: {ColoredOutput.green(final_stats['processed'])}")
    print(f"  å·²ä¸‹è½½èµ„æº: {ColoredOutput.green(final_stats['assets'])}")
    print(f"  å¤±è´¥é¡µé¢: {ColoredOutput.red(final_stats['failed'])}")
    print(f"  æ€»è€—æ—¶: {ColoredOutput.yellow(str(final_stats['elapsed']).split('.')[0])}")
    print(f"  æ€»å…±è®¿é—® URL: {ColoredOutput.yellow(len(scraper_instance.visited))}")
    print("-"*80)
    print(f"  Markdownæ–‡ä»¶ä¿å­˜åœ¨: {ColoredOutput.cyan(scraper_instance.output_dir)}")
    print(f"  å›¾ç‰‡ä¿å­˜åœ¨: {ColoredOutput.cyan(scraper_instance.output_dir / 'images')}")
    print("="*80)


if __name__ == "__main__":
    DEFAULT_URL = "https://iot.mi.com/vela/quickapp/"

    output_dir = "docs"
    delay = 0.3
    workers = 16

    languages = ['zh', 'en']

    print(ColoredOutput.bold(ColoredOutput.blue("ğŸ”§ å°ç±³Velaæ–‡æ¡£çˆ¬å–å·¥å…·")))
    print(ColoredOutput.bold(ColoredOutput.blue("="*50)))

    # ä¸ºæ‰€æœ‰è¯­è¨€ç‰ˆæœ¬å…±äº«ä¸€ä¸ªç»Ÿè®¡å¯¹è±¡
    shared_stats = RealTimeStats()

    for lang in languages:
        lang_url = f"{DEFAULT_URL}{lang}/"
        print(f"\n{ColoredOutput.bold(f'--- ğŸŒ å¼€å§‹çˆ¬å– {lang.upper()} ç‰ˆæœ¬ ({lang_url}) ---')}")
        
        scraper = MarkdownScraper(
            base_url=lang_url,
            output_dir=output_dir,
            stats=shared_stats # ä¼ é€’å…±äº«çš„ç»Ÿè®¡å¯¹è±¡
        )
        run_crawler_with_realtime_stats(
            scraper_instance=scraper,
            start_url=lang_url,
            max_workers=workers,
            delay=delay
        )
        
        print(f"{ColoredOutput.bold(f'--- âœ… {lang.upper()} ç‰ˆæœ¬çˆ¬å–å®Œæˆ ---')}\n")

    print("\n" + "="*80)
    print(ColoredOutput.bold(ColoredOutput.magenta("ğŸ‰ æ‰€æœ‰è¯­è¨€ç‰ˆæœ¬çˆ¬å–å®Œæˆï¼")))
    print("="*80)
    final_stats = shared_stats.get_stats()
    print(f"  æ€»è®¡å·²å¤„ç†é¡µé¢: {ColoredOutput.green(final_stats['processed'])}")
    print(f"  æ€»è®¡å·²ä¸‹è½½èµ„æº: {ColoredOutput.green(final_stats['assets'])}")
    print(f"  æ€»è®¡å¤±è´¥é¡µé¢: {ColoredOutput.red(final_stats['failed'])}")
    print(f"  æ€»è€—æ—¶: {ColoredOutput.yellow(str(final_stats['elapsed']).split('.')[0])}")
    print(f"  æ€»è¾“å‡ºç›®å½•: {ColoredOutput.cyan(output_dir)}")
    print(f"  ä¸­æ–‡ç‰ˆæ–‡ä»¶: {ColoredOutput.cyan(output_dir + '/zh')}")
    print(f"  è‹±æ–‡ç‰ˆæ–‡ä»¶: {ColoredOutput.cyan(output_dir + '/en')}")
    print(f"  ä¸­æ–‡ç‰ˆå›¾ç‰‡: {ColoredOutput.cyan(output_dir + '/zh/images')}")
    print(f"  è‹±æ–‡ç‰ˆå›¾ç‰‡: {ColoredOutput.cyan(output_dir + '/en/images')}")
    print("="*80)

